{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_30280\\442366915.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "import openai\n",
    "import matplotlib.image as img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 101000 files belonging to 101 classes.\n",
      "Using 80800 files for training.\n",
      "Found 101000 files belonging to 101 classes.\n",
      "Using 20200 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory ='C:/Users/User/Allergy-Pal/kaggle/input/food41/images',\n",
    "    image_size = (224, 224),\n",
    "    validation_split = 0.2,\n",
    "    subset = \"training\",\n",
    "    seed = 42,\n",
    "    color_mode = 'rgb',\n",
    "    shuffle = True , \n",
    "    label_mode=\"categorical\" , \n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory ='C:/Users/User/Allergy-Pal/kaggle/input/food41/images',\n",
    "    image_size = (224, 224),\n",
    "    validation_split = 0.2,\n",
    "    subset = \"validation\",\n",
    "    seed = 42,\n",
    "    color_mode = 'rgb',\n",
    "    label_mode=\"categorical\" , \n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheesecake', 'cheese_plate', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon', 'guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']\n"
     ]
    }
   ],
   "source": [
    "classes = []\n",
    "with open(\"C:/Users/User/Allergy-Pal/kaggle/input/food41/meta/meta/classes.txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        classes.append(line.split(\"\\n\")[0]) \n",
    "\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (180 , 180)\n",
    "batch_size = 32\n",
    "\n",
    "callback_list=[\n",
    "    callbacks.EarlyStopping(monitor=\"val_accuracy\",patience=10,restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(factor=0.8,monitor=\"val_accuracy\",patience=3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.EfficientNetB0(include_top = False , input_shape=(224 , 224 , 3))  \n",
    "\n",
    "base_model.trainable = False \n",
    "\n",
    "inputs = tf.keras.layers.Input(shape = (224 , 224 , 3) , name = \"Input_layer\")\n",
    "x = base_model(inputs)\n",
    "x = tf.keras.layers.Dense(101)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "outputs = tf.keras.layers.Dense(101 , \n",
    "                                activation = \"softmax\" , \n",
    "                                name = \"Output_layer\")(x) \n",
    "Model_1 = tf.keras.Model(inputs , outputs) \n",
    "\n",
    "Model_1.compile(\n",
    "    loss = tf.keras.losses.categorical_crossentropy ,\n",
    "    optimizer = tf.keras.optimizers.Adam() , \n",
    "    metrics = ['accuracy'] \n",
    ")\n",
    "\n",
    "Model_1_History = Model_1.fit(train_ds \n",
    "                              , validation_data = val_ds ,\n",
    "                              epochs = 6  ,\n",
    "                              verbose = 1 ,\n",
    "                              callbacks = callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Predicted Class Index: 3\n",
      "Confidence: 2.65%\n",
      "Predicted Class: beef_carpaccio\n"
     ]
    }
   ],
   "source": [
    "Model_1.save(\"C:/Users/User/Allergy-Pal/model.keras\")\n",
    "\n",
    "img = tf.keras.utils.load_img(\n",
    "    \"C:/Users/User/Downloads/pizza12.jpg\", target_size=(224, 224)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) \n",
    "\n",
    "\n",
    "predictions = Model_1.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "print(predictions)\n",
    "print(np.argmax(predictions[0]))\n",
    "print(np.max(predictions[0]))\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f}% percent confidence.\"\n",
    "    .format(classes[np.argmax(score)], 100 * np.max(predictions[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "2525/2525 [==============================] - 1027s 405ms/step - loss: 0.8292 - accuracy: 0.8861 - val_loss: 8.2527 - val_accuracy: 0.5792\n",
      "Epoch 6/10\n",
      "2525/2525 [==============================] - 1450s 574ms/step - loss: 0.5289 - accuracy: 0.9152 - val_loss: 8.0561 - val_accuracy: 0.5788\n",
      "Epoch 7/10\n",
      "2525/2525 [==============================] - 1470s 582ms/step - loss: 0.3910 - accuracy: 0.9326 - val_loss: 7.7413 - val_accuracy: 0.5811\n",
      "Epoch 8/10\n",
      "2525/2525 [==============================] - 1515s 600ms/step - loss: 0.3214 - accuracy: 0.9429 - val_loss: 7.7662 - val_accuracy: 0.5874\n",
      "Epoch 9/10\n",
      "2525/2525 [==============================] - 1095s 434ms/step - loss: 0.2873 - accuracy: 0.9485 - val_loss: 7.7130 - val_accuracy: 0.5907\n",
      "Epoch 10/10\n",
      "2525/2525 [==============================] - 1126s 446ms/step - loss: 0.2400 - accuracy: 0.9548 - val_loss: 7.7250 - val_accuracy: 0.5908\n"
     ]
    }
   ],
   "source": [
    "Model_1 = tf.keras.models.load_model(\"C:/Users/User/Allergy-Pal/model.keras\")\n",
    "\n",
    "base_model.trainable = True\n",
    "\n",
    "for layer in base_model.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "Model_1.compile(loss = \"categorical_crossentropy\" ,\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate= 0.0001), \n",
    "                metrics = [\"accuracy\"]\n",
    "               )\n",
    "\n",
    "initial_epoch = 5\n",
    "\n",
    "Fine_Tune_epoch = initial_epoch + 5\n",
    "\n",
    "Stage_2_history = Model_1.fit(train_ds ,\n",
    "                              epochs = Fine_Tune_epoch , \n",
    "                              validation_data = val_ds ,\n",
    "                              validation_steps = len(val_ds) ,\n",
    "                              initial_epoch = initial_epoch-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "This image most likely belongs to pizza with a 100.00% percent confidence.\n"
     ]
    }
   ],
   "source": [
    "Model_1.save(\"C:/Users/User/Allergy-Pal/model.keras\")\n",
    "\n",
    "img = tf.keras.utils.load_img(\n",
    "    \"C:/Users/User/Downloads/pizza123.jpg\", target_size=(224, 224)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) \n",
    "\n",
    "\n",
    "predictions = Model_1.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f}% percent confidence.\"\n",
    "    .format(classes[np.argmax(score)], 100 * np.max(predictions[0]))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
